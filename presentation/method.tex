\begin{frame}
\frametitle{Fusion Methods}
Goal: Reconstruct a global model $\mathbb{M} = (\hat{V}, \hat{N})$ from
a sequence of depth frames $D_i = (V_i, N_i)$. \\

Assuming that the scene is static, depth frames can be fused into a single
point cloud by finding the camera pose which aligns the current depth frame
with the model. \\

Outlier points which do not fit in the model should be presumed errors and discarded.
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Challenges}
Measurement
\begin{itemize}
  \item High volume of data (640x480 @ 30fps = 9 million points per sec)
  \item Occlusion (stuff in the way), holes
  \item Measurement errors: incident angles, shiny or transparent materials
  \item Potentially erratic camera movement: blurry measurements
  \item Dynamic scenes, moving objects
  \item Camera drift: accumulation of errors in pose estimation
\end{itemize}
Video: playing with RealSense PointCloud example: \url{https://youtu.be/cQrPQ1dFIYU}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{Frameworks}
Fusion and alignment algorithms are bundled in a few frameworks:
\begin{itemize}
  \item Robot Operating System (ROS)
  \item Point Cloud Library
  \item OpenCV (Kinfu)
\end{itemize}

Intel RealSense SDK provides ``wrappers'' for these frameworks.

Idea: encapsulate sensing device so that a stream (or pipe) can be fed into
one of these frameworks' algorithms.

\url{https://github.com/IntelRealSense/librealsense/blob/master/wrappers/opencv/kinfu/rs-kinfu.cpp}
\end{frame}
